{
    "models": [
        {
            "model": "/app/data/models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf",
            "model_alias": "Meta-Llama-3.1-8B-Instruct-Q6_K",
            "n_gpu_layers": 99,
            "n_batch": 8192,
            "n_ctx": 32768
        },
        {
            "model": "/app/data/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
            "model_alias": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
            "n_gpu_layers": 99,
            "n_batch": 8192,
            "n_ctx": 32768
        },
        {
            "model": "/app/data/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf",
            "model_alias": "Meta-Llama-3-8B-Instruct.Q5_K_M",
            "n_gpu_layers": 99,
            "n_batch": 8192,
            "n_ctx": 32768
        },
        {
            "model": "/app/data/models/llama-2-13b-chat.Q4_K_M.gguf",
            "model_alias": "llama-2-13b-chat.Q4_K_M",
            "n_gpu_layers": 99,
            "n_batch": 8192,
            "n_ctx": 32768
        },
        {
            "model": "/app/data/models/Lexi-Llama-3-8B-Uncensored-Q6_K.gguf",
            "model_alias": "Lexi-Llama-3-8B-Uncensored-Q6_K",
            "n_gpu_layers": 99,
            "n_batch": 8192,
            "n_ctx": 32768
        }
    ],
    "host": "0.0.0.0",
    "port": 8000
}